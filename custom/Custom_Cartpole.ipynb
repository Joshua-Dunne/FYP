{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6494254",
   "metadata": {},
   "source": [
    "### An edit of [Cartpole-V1](https://gym.openai.com/envs/CartPole-v1/), where I look to introduce a slight amount of randomness in the form of the Environment randomly increasing the angular velocity of the Pole, making it lean harder in one direction so the Agent has to move to catch it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de5206d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Classic cart-pole system implemented by Rich Sutton et al.\n",
    "Copied from http://incompleteideas.net/sutton/book/code/pole.c\n",
    "permalink: https://perma.cc/C9ZM-652R\n",
    "\"\"\"\n",
    "import os\n",
    "import math\n",
    "from typing import Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pygame\n",
    "from pygame import gfxdraw\n",
    "\n",
    "import gym\n",
    "from gym import spaces, logger\n",
    "from gym.utils import seeding\n",
    "\n",
    "# Stable Baselines 3 for Reinforcement Learning\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Allows us to generate a new amount of time to wait before applying randomness\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a5eb79",
   "metadata": {},
   "source": [
    "   This environment corresponds to the version of the cart-pole problem\n",
    "    described by Barto, Sutton, and Anderson in [\"Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem\"](https://ieeexplore.ieee.org/document/6313077).\n",
    "   A pole is attached by an un-actuated joint to a cart, which moves along a\n",
    "    frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart.\n",
    "\n",
    "   ### Action Space\n",
    "\n",
    "   The action is a `ndarray` with shape `(1,)` which can take values `{0, 1}` indicating the direction of the fixed force the cart is pushed with.\n",
    "\n",
    "    | Num | Action                 |\n",
    "    |-----|------------------------|\n",
    "    | 0   | Push cart to the left  |\n",
    "    | 1   | Push cart to the right |\n",
    "\n",
    "   **Note**: The velocity that is reduced or increased by the applied force is not fixed and it depends on the angle the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it\n",
    "\n",
    "   ### Observation Space\n",
    "\n",
    "   The observation is a `ndarray` with shape `(4,)` with the values corresponding to the following positions and velocities:\n",
    "\n",
    "    | Num | Observation           | Min                  | Max                |\n",
    "    |-----|-----------------------|----------------------|--------------------|\n",
    "    | 0   | Cart Position         | -4.8                 | 4.8                |\n",
    "    | 1   | Cart Velocity         | -Inf                 | Inf                |\n",
    "    | 2   | Pole Angle            | ~ -0.418 rad (-24°)  | ~ 0.418 rad (24°)  |\n",
    "    | 3   | Pole Angular Velocity | -Inf                 | Inf                |\n",
    "\n",
    "   **Note:** While the ranges above denote the possible values for observation space of each element, it is not reflective of the allowed values of the state space in an unterminated episode. Particularly:\n",
    "    -  The cart x-position (index 0) can be take values between `(-4.8, 4.8)`, but the episode terminates if the cart leaves the `(-2.4, 2.4)` range.\n",
    "    -  The pole angle can be observed between  `(-.418, .418)` radians (or **±24°**), but the episode terminates if the pole angle is not in the range `(-.2095, .2095)` (or **±12°**)\n",
    "\n",
    "   ### Rewards\n",
    "\n",
    "   Since the goal is to keep the pole upright for as long as possible, a reward of `+1` for every step taken, including the termination step, is allotted. The threshold for rewards is 475 for v1.\n",
    "\n",
    "   ### Starting State\n",
    "\n",
    "   All observations are assigned a uniformly random value in `(-0.05, 0.05)`\n",
    "\n",
    "   ### Episode Termination\n",
    "\n",
    "   The episode terminates if any one of the following occurs:\n",
    "   1. Pole Angle is greater than ±12°\n",
    "   2. Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n",
    "   3. Episode length is greater than 500 (200 for v0)\n",
    "\n",
    "   ### Arguments\n",
    "\n",
    "   gym.make('CartPole-v1')\n",
    "\n",
    "   No additional arguments are currently supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddb173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleEnv():\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 50}\n",
    "\n",
    "    def __init__(self, randomize):\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = self.masspole + self.masscart\n",
    "        self.length = 0.5  # actually half the pole's length\n",
    "        self.polemass_length = self.masspole * self.length\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "        self.kinematics_integrator = \"euler\"\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        self.theta_threshold_radians = 15 * math.pi / 180\n",
    "        self.x_threshold = 2.4\n",
    "\n",
    "        # Angle limit set to 2 * theta_threshold_radians so failing observation\n",
    "        # is still within bounds.\n",
    "        high = np.array(\n",
    "            [\n",
    "                self.x_threshold * 2,\n",
    "                np.finfo(np.float32).max,\n",
    "                self.theta_threshold_radians * 2,\n",
    "                np.finfo(np.float32).max,\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "        self.isopen = True\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_done = None\n",
    "        \n",
    "        # the first random push to the angular velocity of the pole will always happen at a preset amount of steps\n",
    "        self.randomSteps = 100\n",
    "        # flash the pole a color when randomness is applied\n",
    "        self.randomApplied = False\n",
    "        # should the environment use the above randomness\n",
    "        self.applyRandomness = randomize\n",
    "\n",
    "    def step(self, action):\n",
    "        err_msg = f\"{action!r} ({type(action)}) invalid\"\n",
    "        assert self.action_space.contains(action), err_msg\n",
    "        assert self.state is not None, \"Call reset before using step method.\"\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        force = self.force_mag if action == 1 else -self.force_mag\n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "\n",
    "        # For the interested reader:\n",
    "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "        temp = (\n",
    "            force + self.polemass_length * theta_dot ** 2 * sintheta\n",
    "        ) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / (\n",
    "            self.length * (4.0 / 3.0 - self.masspole * costheta ** 2 / self.total_mass)\n",
    "        )\n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\n",
    "        if self.kinematics_integrator == \"euler\":\n",
    "            x = x + self.tau * x_dot\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "        else:  # semi-implicit euler\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            x = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "        \n",
    "        if self.applyRandomness:\n",
    "            self.randomSteps = self.randomSteps - 1 # for each step, decrease the steps to next randomness\n",
    "\n",
    "            if self.randomSteps < 0:\n",
    "                theta_dot *= 4 # theta_dot is the angular velocity of the pole\n",
    "                # when randomness is required, we will increase the velocity,\n",
    "                # so that the AI has to account for the random speed increase\n",
    "                self.randomSteps = random.randint(50,80)\n",
    "                self.randomApplied = True\n",
    "\n",
    "        self.state = (x, x_dot, theta, theta_dot)\n",
    "\n",
    "        done = bool(\n",
    "            x < -self.x_threshold\n",
    "            or x > self.x_threshold\n",
    "            or theta < -self.theta_threshold_radians\n",
    "            or theta > self.theta_threshold_radians\n",
    "        )\n",
    "\n",
    "        if not done:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_done is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_done = 0\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            if self.steps_beyond_done == 0:\n",
    "                logger.warn(\n",
    "                    \"You are calling 'step()' even though this \"\n",
    "                    \"environment has already returned done = True. You \"\n",
    "                    \"should always call 'reset()' once you receive 'done = \"\n",
    "                    \"True' -- any further steps are undefined behavior.\"\n",
    "                )\n",
    "            self.steps_beyond_done += 1\n",
    "            reward = 0.0\n",
    "\n",
    "        return np.array(self.state, dtype=np.float32), reward, done, {}\n",
    "\n",
    "    def reset(\n",
    "        self,\n",
    "        *,\n",
    "        seed: Optional[int] = None,\n",
    "        return_info: bool = False,\n",
    "        options: Optional[dict] = None,\n",
    "    ):\n",
    "        #super().reset(seed=seed)\n",
    "        self.state = np.random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "        self.steps_beyond_done = None\n",
    "        if not return_info:\n",
    "            return np.array(self.state, dtype=np.float32)\n",
    "        else:\n",
    "            return np.array(self.state, dtype=np.float32), {}\n",
    "        \n",
    "        self.randomSteps = 100 # reset the initial wait\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        screen_width = 600\n",
    "        screen_height = 400\n",
    "\n",
    "        world_width = self.x_threshold * 2\n",
    "        scale = screen_width / world_width\n",
    "        polewidth = 10.0\n",
    "        polelen = scale * (2 * self.length)\n",
    "        cartwidth = 50.0\n",
    "        cartheight = 30.0\n",
    "\n",
    "        if self.state is None:\n",
    "            return None\n",
    "\n",
    "        x = self.state\n",
    "\n",
    "        if self.screen is None:\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.screen = pygame.display.set_mode((screen_width, screen_height))\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        self.surf = pygame.Surface((screen_width, screen_height))\n",
    "        self.surf.fill((255, 255, 255))\n",
    "\n",
    "        l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2\n",
    "        axleoffset = cartheight / 4.0\n",
    "        cartx = x[0] * scale + screen_width / 2.0  # MIDDLE OF CART\n",
    "        carty = 100  # TOP OF CART\n",
    "        cart_coords = [(l, b), (l, t), (r, t), (r, b)]\n",
    "        cart_coords = [(c[0] + cartx, c[1] + carty) for c in cart_coords]\n",
    "        gfxdraw.aapolygon(self.surf, cart_coords, (0, 0, 0))\n",
    "        gfxdraw.filled_polygon(self.surf, cart_coords, (0, 0, 0))\n",
    "\n",
    "        l, r, t, b = (\n",
    "            -polewidth / 2,\n",
    "            polewidth / 2,\n",
    "            polelen - polewidth / 2,\n",
    "            -polewidth / 2,\n",
    "        )\n",
    "\n",
    "        pole_coords = []\n",
    "        for coord in [(l, b), (l, t), (r, t), (r, b)]:\n",
    "            coord = pygame.math.Vector2(coord).rotate_rad(-x[2])\n",
    "            coord = (coord[0] + cartx, coord[1] + carty + axleoffset)\n",
    "            pole_coords.append(coord)\n",
    "        \n",
    "        if self.randomApplied:\n",
    "            gfxdraw.aapolygon(self.surf, pole_coords, (255, 0, 0))\n",
    "            gfxdraw.filled_polygon(self.surf, pole_coords, (255, 0, 0))\n",
    "            self.randomApplied = False\n",
    "        else:\n",
    "            gfxdraw.aapolygon(self.surf, pole_coords, (202, 152, 101))\n",
    "            gfxdraw.filled_polygon(self.surf, pole_coords, (202, 152, 101))\n",
    "\n",
    "        gfxdraw.aacircle(\n",
    "            self.surf,\n",
    "            int(cartx),\n",
    "            int(carty + axleoffset),\n",
    "            int(polewidth / 2),\n",
    "            (129, 132, 203),\n",
    "        )\n",
    "        gfxdraw.filled_circle(\n",
    "            self.surf,\n",
    "            int(cartx),\n",
    "            int(carty + axleoffset),\n",
    "            int(polewidth / 2),\n",
    "            (129, 132, 203),\n",
    "        )\n",
    "\n",
    "        gfxdraw.hline(self.surf, 0, screen_width, carty, (0, 0, 0))\n",
    "\n",
    "        self.surf = pygame.transform.flip(self.surf, False, True)\n",
    "        self.screen.blit(self.surf, (0, 0))\n",
    "        if mode == \"human\":\n",
    "            pygame.event.pump()\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "            pygame.display.flip()\n",
    "\n",
    "        if mode == \"rgb_array\":\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)\n",
    "            )\n",
    "        else:\n",
    "            return self.isopen\n",
    "\n",
    "    def close(self):\n",
    "        if self.screen is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "            self.isopen = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45d62cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CartPoleEnv(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c689645a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to test that the Environment works properly\n",
    "# This will randomly pick from a Discrete action step, no model is used here.\n",
    "\n",
    "pygame.init()\n",
    "win = pygame.display.set_mode((600,400))\n",
    "pygame.display.set_caption(\"Cartpole with Randomness\")\n",
    "\n",
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        pygame.event.get()\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score += reward         \n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff72d01",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fd87d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Directories To Save Logs (and later Models)\n",
    "log_path = os.path.join('training', 'logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ed1d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CartPoleEnv(True)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# Multilayer Perceptron Policy\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2c0484",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=80000) # change this to code block if you wish to train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02941bc5",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bf0caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_Path = os.path.join('training', 'saved_models', 'customCartpole_PPO_Random_80000') # path to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a10b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(PPO_Path) # save the model to our path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda27fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(model) # delete the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fca7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(PPO_Path, env=env) # load model from path specified above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1919d97",
   "metadata": {},
   "source": [
    "### Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12de6d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CartPoleEnv(True) # reset environment for model running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c33092a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pygame.init()\n",
    "win = pygame.display.set_mode((600,400))\n",
    "pygame.display.set_caption(\"Cartpole with Randomness\")\n",
    "\n",
    "episodes = 3\n",
    "for episode in range(1, episodes + 1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render() # renders environment\n",
    "        action, _ = model.predict(obs) # predict returns two, but we only require action\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        \n",
    "        if(score > 500):\n",
    "            done = True\n",
    "        \n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close() # Close environment, can run outside of cell to also close environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa222e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feee10f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
