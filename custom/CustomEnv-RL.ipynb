{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d303a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pygame installation\n",
    "!pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7dd6081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.0 (SDL 2.0.16, Python 3.9.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame as pg\n",
    "from pygame import image as img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81d2acc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gym Imports\n",
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box, Dict, Tuple, MultiBinary, MultiDiscrete # different types of spaces\n",
    "\n",
    "# Helpers\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Stable baselines\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Time module to make program halt for presentation purposes\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "642eff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definitions for use\n",
    "def load_image(file):\n",
    "    \"\"\"loads an image, prepares it for play\"\"\"\n",
    "    try:\n",
    "        surface = pg.image.load(file)\n",
    "    except pg.error:\n",
    "        raise SystemExit('Could not load image \"%s\" %s' % (file, pg.get_error()))\n",
    "    return surface.convert_alpha() # convert_alpha allows for transparency from .pngs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a47be1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg.init()\n",
    "win = pg.display.set_mode((0,0))\n",
    "pg.display.set_caption(\"Custom Environment - Guess Path\")\n",
    "\n",
    "class Robot(pg.sprite.Sprite):\n",
    "    \"\"\"Visual for AI space and movement direction\"\"\"\n",
    "    images = [load_image(\"assets/sprites/robo_R.png\"), \n",
    "              load_image(\"assets/sprites/robo_L.png\"), \n",
    "              load_image(\"assets/sprites/robo_U.png\"),\n",
    "              load_image(\"assets/sprites/robo_D.png\")]\n",
    "\n",
    "    def __init__(self, pos):\n",
    "        pg.sprite.Sprite.__init__(self, self.containers)\n",
    "        self.image = self.images[0]\n",
    "        self.rect = self.image.get_rect()\n",
    "        self.rect.x = pos[0] - (self.rect.width / 2.0)\n",
    "        self.rect.y = pos[1] - (self.rect.height / 2.0)\n",
    "\n",
    "    def update(self, action: int):\n",
    "        if action >= 0:\n",
    "            self.image = self.images[action]\n",
    "    \n",
    "    def setPosition(self, newPos):\n",
    "        self.rect = self.image.get_rect()\n",
    "        self.rect.x = newPos[0] - (self.rect.width / 2.0)\n",
    "        self.rect.y = newPos[1] - (self.rect.height / 2.0)\n",
    "\n",
    "class Space(pg.sprite.Sprite):\n",
    "    \"\"\"Visual for space type. Normal, positive or negative\"\"\"\n",
    "    images = [load_image(\"assets/sprites/normal.png\"), \n",
    "              load_image(\"assets/sprites/plus.png\"), \n",
    "              load_image(\"assets/sprites/minus.png\"),\n",
    "              load_image(\"assets/sprites/start.png\"), \n",
    "              load_image(\"assets/sprites/goal.png\")]\n",
    "    spaceType = 0\n",
    "\n",
    "    def __init__(self, pos, jumpDistance):\n",
    "        pg.sprite.Sprite.__init__(self, self.containers)\n",
    "        \n",
    "        self.jumpDistance = jumpDistance\n",
    "        \n",
    "        if(jumpDistance == 0):  # neutral movement - does not push Robot\n",
    "            self.image = self.images[0]\n",
    "        elif(jumpDistance > 0): # positive movement - pushes robot forward\n",
    "            self.image = self.images[1]\n",
    "        else:                    # negative movement - pushes robot backward\n",
    "            self.image = self.images[2]\n",
    "\n",
    "        self.rect = self.image.get_rect()\n",
    "        self.rect.x = pos[0] - (self.rect.width / 2.0)\n",
    "        self.rect.y = pos[1] - (self.rect.height / 2.0)\n",
    "        \n",
    "    def setType(self, spaceType):\n",
    "            self.image = self.images[spaceType]\n",
    "            self.spaceType = spaceType\n",
    "            \n",
    "            if spaceType == 3:\n",
    "                self.spaceType = 0 # if the current space type is the start, set it to a nothing type\n",
    "            \n",
    "            if spaceType == 1:\n",
    "                self.jumpDistance = 2\n",
    "            elif spaceType == 2:\n",
    "                self.jumpDistance = -2\n",
    "\n",
    "# Initialize Game Groups\n",
    "all = pg.sprite.RenderUpdates()\n",
    "    \n",
    "Space.containers = all\n",
    "Robot.containers = all\n",
    "\n",
    "# Board does not require rendering\n",
    "        \n",
    "class Board():\n",
    "    \"\"\"Board that sets up and displays all spaces\"\"\"\n",
    "    def __init__(self, maxCols, pos):\n",
    "        self.spaces = [[], [], [], [], []]\n",
    "        \n",
    "        self.maxCols = maxCols\n",
    "        \n",
    "        if self.maxCols < 1:\n",
    "            self.maxCols = 1 # cap minimum value to max rows in case of emergency\n",
    "        \n",
    "        self.middle = (len(self.spaces) // 2)\n",
    "        self.playerPos = [0,self.middle]\n",
    "        self.goal = [self.middle, self.maxCols - 1]\n",
    "        \n",
    "        # Thanks to Jack Malone for help with this loop\n",
    "        for index, value in enumerate(self.spaces):\n",
    "                    for y in range(self.maxCols):\n",
    "                            value.append(Space(((64 * y) + pos[0], (64 * index) + pos[1]), 0))\n",
    "                        \n",
    "    def setStartEnd(self):\n",
    "        self.spaces[self.playerPos[1]][self.playerPos[0]].setType(3)\n",
    "        self.spaces[self.goal[0]][self.goal[1]].setType(4)\n",
    "                        \n",
    "    def update(self, robot: Robot, action: int):\n",
    "        additionalReward = 0 # calculate reward for robot here\n",
    "        \n",
    "        if action >= 0:\n",
    "            if action == 0: # right\n",
    "                  if self.playerPos[0] < self.maxCols - 1:\n",
    "                        self.playerPos[0] += 1\n",
    "            elif action == 1: # left\n",
    "                 if self.playerPos[0] > 0:\n",
    "                        self.playerPos[0] -= 1\n",
    "            elif action == 2: # up\n",
    "                if self.playerPos[1] > 0:\n",
    "                        self.playerPos[1] -= 1\n",
    "            elif action == 3: # down\n",
    "                if self.playerPos[1] < len(self.spaces) - 1:\n",
    "                        self.playerPos[1] += 1\n",
    "            \n",
    "            # first check to see if the position has to be updated\n",
    "            # as the robot may move onto a push space\n",
    "            \n",
    "            pushValue = self.spaces[self.playerPos[1]][self.playerPos[0]].jumpDistance\n",
    "            \n",
    "            if pushValue > 0:\n",
    "                additionalReward = 2\n",
    "            elif pushValue < 0:\n",
    "                additionalReward = -2\n",
    "            \n",
    "            if pushValue != 0: # only do push calculations if the robot has to be pushed\n",
    "                self.playerPos[0] += pushValue\n",
    "\n",
    "                # now check to see if the player has jumped outside the bounds of the board\n",
    "                # only generate reward for a successful jump\n",
    "                if self.playerPos[0] > self.maxCols - 1:\n",
    "                    self.playerPos[0] = self.maxCols - 1\n",
    "                    additionalReward = 0\n",
    "                elif self.playerPos[0] < 0:\n",
    "                    self.playerPos[0] = 0\n",
    "                    additionalReward = 0\n",
    "                \n",
    "            # now that the robot has moved, update it's position\n",
    "            robot.setPosition(((self.playerPos[0] * 64) + 96, (self.playerPos[1] * 64) + 96))\n",
    "            \n",
    "            return additionalReward\n",
    "            \n",
    "    def initializeSpaces(self):\n",
    "        self.spaces[self.middle][3].setType(1)\n",
    "        self.spaces[0][4].setType(2)\n",
    "        self.spaces[1][4].setType(2)\n",
    "        self.spaces[2][4].setType(2)\n",
    "        self.spaces[3][4].setType(2)\n",
    "        self.spaces[4][4].setType(2)\n",
    "        \n",
    "        self.spaces[0][7].setType(1)\n",
    "        self.spaces[0][8].setType(2)\n",
    "        self.spaces[1][8].setType(2)\n",
    "        self.spaces[2][8].setType(2)\n",
    "        self.spaces[3][8].setType(2)\n",
    "        self.spaces[4][8].setType(2)\n",
    "        \n",
    "        self.spaces[3][11].setType(1)\n",
    "        self.spaces[0][12].setType(2)\n",
    "        self.spaces[1][12].setType(2)\n",
    "        self.spaces[2][12].setType(2)\n",
    "        self.spaces[3][12].setType(2)\n",
    "        self.spaces[4][12].setType(2)\n",
    "        \n",
    "    def winCheck(self):\n",
    "        if(self.playerPos[1] == self.goal[0] \n",
    "           and self.playerPos[0] == self.goal[1]):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "pg.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7bbb078",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathEnv(Env):\n",
    "    win = pg.display.set_mode((1600,800))\n",
    "    pg.display.set_caption(\"Custom Environment - Guess Path RL\")\n",
    "    bg = load_image('assets/sprites/background.png')\n",
    "    board = Board(16, (100,100))\n",
    "    board.setStartEnd()\n",
    "    board.initializeSpaces()\n",
    "    robo = Robot(((board.playerPos[0] * 64) + 96, \n",
    "              (board.playerPos[1] * 64) + 96))\n",
    "    action = -1\n",
    "    clock = pg.time.Clock()\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Actions: 0 - Left, 1 - Up, 2 - Right, 3 - Down\n",
    "        self.action_space = Discrete(4)\n",
    "        \n",
    "        # Create observation space\n",
    "        self.observation_space = Discrete(80)\n",
    "        \n",
    "        # Determine starting state upon initialization\n",
    "        self.state = self.determineState()\n",
    "        # \n",
    "        self.alloted_length = 600\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        self.clock.tick(30)\n",
    "        \n",
    "        # Update Environment elements\n",
    "        self.robo.update(action)\n",
    "        \n",
    "        additionalReward = self.board.update(self.robo, action)\n",
    "        \n",
    "        # With elements updated, determine state, reward etc\n",
    "        self.state = self.determineState()\n",
    "        \n",
    "        # Reduce alloted length to use environment by time moved via pygame clock\n",
    "        self.alloted_length -= self.clock.get_time()\n",
    "\n",
    "        # TODO: Calculate reward\n",
    "        reward = -1 + additionalReward\n",
    "        \n",
    "        # Check if environment is done\n",
    "        if self.alloted_length <= 0: \n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "            \n",
    "        if self.board.winCheck():\n",
    "            reward += 30 # when finding the goal, end the environment and give a good reward\n",
    "            done = True\n",
    "        \n",
    "        # Needed during the return\n",
    "        info = {}\n",
    "  \n",
    "        # Return step information\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def render(self):\n",
    "        win.blit(self.bg, (0,0))\n",
    "        for r in self.board.spaces:\n",
    "                for c in r: # position all spaces to the correct top left position\n",
    "                    win.blit(c.image, (c.rect.x, c.rect.y))\n",
    "\n",
    "        win.blit(self.robo.image, (self.robo.rect.x, self.robo.rect.y))\n",
    "        pg.display.update()\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset game elements\n",
    "        \n",
    "        self.board.playerPos = [0,self.board.middle] # reset player pos on board\n",
    "        \n",
    "        self.robo.setPosition(((self.board.playerPos[0] * 64) + 96, \n",
    "              (self.board.playerPos[1] * 64) + 96)) # place robo onto the correct spot\n",
    "        \n",
    "        # Reset starting state\n",
    "        self.state = self.determineState()\n",
    "        # Reset alloted time to interact\n",
    "        self.alloted_length = 600\n",
    "        return self.state\n",
    "    \n",
    "    def calculateReward(self):\n",
    "        # calculate reward based for AI\n",
    "        pass\n",
    "    \n",
    "    def determineState(self):\n",
    "        # the current state is where the player currently is on the board\n",
    "        \n",
    "        # since we calculate the current position by multiplying the two\n",
    "        # x and y positions on our array together,\n",
    "        # we want to make sure we do not multiply by 0,\n",
    "        # as this will always set the state to 0.\n",
    "        \n",
    "        if self.board.playerPos[0] == 0:    # if the x position is 0,\n",
    "            state = self.board.playerPos[1] # set the state to be just the y position\n",
    "        elif self.board.playerPos[1] == 0:    # otherwise if the y position is 0\n",
    "            state = self.board.playerPos[0] # then set the state to be just the x position\n",
    "        else:# if both positions are not 0, then the state is both positions multiplied together   \n",
    "            state = self.board.playerPos[0] * self.board.playerPos[1]\n",
    "        \n",
    "        return state\n",
    "    \n",
    "pg.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e7400fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PathEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6685d543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31316096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to test that the Environment works properly\n",
    "# This will randomly pick from a Discrete action step, no model is used here.\n",
    "\n",
    "pg.init()\n",
    "win = pg.display.set_mode((1600,800))\n",
    "pg.display.set_caption(\"Custom Environment - Guess Path RL\")\n",
    "\n",
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        pg.event.get()\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score += reward         \n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()\n",
    "pg.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34bba63",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg.quit() # optional pygame quit in case of error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bfa67a",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b9d110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('training', 'logs')\n",
    "training_log_path = os.path.join(log_path, 'GuessPathPPO_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28033750",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "feafa7c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to training\\logs\\PPO_6\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.9     |\n",
      "|    ep_rew_mean     | -14.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 67       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -13.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 30          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 136         |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003928896 |\n",
      "|    clip_fraction        | 0.0482      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.352      |\n",
      "|    explained_variance   | 0.373       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.73        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00783    |\n",
      "|    value_loss           | 13.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 18.1        |\n",
      "|    ep_rew_mean          | -13         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 29          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 206         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003089864 |\n",
      "|    clip_fraction        | 0.0478      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.344      |\n",
      "|    explained_variance   | 0.403       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.27        |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00774    |\n",
      "|    value_loss           | 13.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 17.9         |\n",
      "|    ep_rew_mean          | -11.6        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 29           |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 276          |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028419904 |\n",
      "|    clip_fraction        | 0.052        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.332       |\n",
      "|    explained_variance   | 0.399        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.98         |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.00892     |\n",
      "|    value_loss           | 12.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 17.5        |\n",
      "|    ep_rew_mean          | 1.4         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 29          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 346         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046568617 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.393      |\n",
      "|    explained_variance   | 0.34        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.49        |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0132     |\n",
      "|    value_loss           | 21.1        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 16.9       |\n",
      "|    ep_rew_mean          | 8.46       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 29         |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 416        |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01351021 |\n",
      "|    clip_fraction        | 0.0889     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.351     |\n",
      "|    explained_variance   | 0.295      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 46.1       |\n",
      "|    n_updates            | 380        |\n",
      "|    policy_gradient_loss | -0.0175    |\n",
      "|    value_loss           | 104        |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 17.1         |\n",
      "|    ep_rew_mean          | 12.1         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 29           |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 485          |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046518766 |\n",
      "|    clip_fraction        | 0.0921       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.3         |\n",
      "|    explained_variance   | 0.254        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 49.7         |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.0194      |\n",
      "|    value_loss           | 107          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 16.4        |\n",
      "|    ep_rew_mean          | 17.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 29          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 554         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007158611 |\n",
      "|    clip_fraction        | 0.0942      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.273      |\n",
      "|    explained_variance   | 0.335       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 38.7        |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0181     |\n",
      "|    value_loss           | 74.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 16.1         |\n",
      "|    ep_rew_mean          | 16.8         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 29           |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 622          |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055052354 |\n",
      "|    clip_fraction        | 0.0769       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.209       |\n",
      "|    explained_variance   | 0.476        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.24         |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.0134      |\n",
      "|    value_loss           | 29.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 15.6         |\n",
      "|    ep_rew_mean          | 19.9         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 29           |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 691          |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033167098 |\n",
      "|    clip_fraction        | 0.0459       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.18        |\n",
      "|    explained_variance   | 0.343        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15           |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.0116      |\n",
      "|    value_loss           | 44.6         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1f33f27e7f0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08287c85",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e2a26702",
   "metadata": {},
   "outputs": [],
   "source": [
    "guesspath_path = os.path.join('training', 'saved_models', 'GuessModel_PPO_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aab457f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(guesspath_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "51cdda47",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(model) # uncomment to delete the model after saving it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "81a86806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = PPO.load(guesspath_path, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ada954",
   "metadata": {},
   "source": [
    "# Watch Model\n",
    "### With a loaded model, it will be used with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5bfcf30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PathEnv() # set environment if not set already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9195ba95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-1\n",
      "Episode:2 Score:21\n",
      "Episode:3 Score:21\n",
      "Episode:4 Score:21\n",
      "Episode:5 Score:21\n",
      "Episode:6 Score:20\n",
      "Episode:7 Score:21\n",
      "Episode:8 Score:21\n",
      "Episode:9 Score:-8\n",
      "Episode:10 Score:21\n",
      "Episode:11 Score:21\n",
      "Episode:12 Score:21\n",
      "Episode:13 Score:21\n",
      "Episode:14 Score:21\n",
      "Episode:15 Score:21\n",
      "Episode:16 Score:15\n",
      "Episode:17 Score:21\n",
      "Episode:18 Score:21\n",
      "Episode:19 Score:20\n",
      "Episode:20 Score:21\n"
     ]
    }
   ],
   "source": [
    "pg.display.init()\n",
    "win = pg.display.set_mode((1600,800))\n",
    "pg.display.set_caption(\"Custom Environment - Guess Path RL\")\n",
    "\n",
    "episodes = 20\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        pg.event.get()\n",
    "        env.render()\n",
    "        action, _ = model.predict(obs) # predict returns two, but we only require action\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score += reward         \n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()\n",
    "pg.display.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4b3343",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
